{"cells":[{"cell_type":"markdown","id":"1df29eda","metadata":{"id":"1df29eda"},"source":["Step 0. Unzip enron1.zip into the current directory."]},{"cell_type":"markdown","id":"bf32cfce","metadata":{"id":"bf32cfce"},"source":["Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."]},{"cell_type":"code","execution_count":11,"id":"20c5d195","metadata":{"id":"20c5d195"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n"]},{"cell_type":"code","execution_count":null,"id":"40cdeb0d","metadata":{},"outputs":[],"source":["\n","def read_spam():\n","    category = 'spam'\n","    directory = './enron1/spam'\n","    return read_category(category, directory)\n","\n","def read_ham():\n","    category = 'ham'\n","    directory = './enron1/ham'\n","    return read_category(category, directory)\n","\n","def read_category(category, directory):\n","    emails = []\n","    for filename in os.listdir(directory):\n","        if not filename.endswith(\".txt\"):\n","            continue\n","        with open(os.path.join(directory, filename), 'r') as fp:\n","            try:\n","                content = fp.read()\n","                emails.append({'name': filename, 'content': content, 'category': category})\n","            except:\n","                print(f'skipped {filename}')\n","    return emails\n"]},{"cell_type":"code","execution_count":16,"id":"3e7ea111","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["skipped 2248.2004-09-23.GP.spam.txt\n","skipped 2526.2004-10-17.GP.spam.txt\n","skipped 2698.2004-10-31.GP.spam.txt\n","skipped 4566.2005-05-24.GP.spam.txt\n"]}],"source":["\n","ham = read_ham()\n","spam = read_spam()\n","\n","\n","df_ham = pd.DataFrame.from_records(ham)\n","df_spam = pd.DataFrame.from_records(spam)\n","df = pd.concat([df_ham, df_spam], ignore_index=True)"]},{"cell_type":"code","execution_count":24,"id":"e2fa6f97","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>content</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001.1999-12-10.farmer.ham.txt</td>\n","      <td>Subject: christmas tree farm pictures\\n</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002.1999-12-13.farmer.ham.txt</td>\n","      <td>Subject: vastar resources , inc .\\ngary , prod...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0003.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: calpine daily gas nomination\\n- calpi...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0004.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: re : issue\\nfyi - see note below - al...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0005.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: meter 7268 nov allocation\\nfyi .\\n- -...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5163</th>\n","      <td>5163.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: our pro - forma invoice attached\\ndiv...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5164</th>\n","      <td>5164.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: str _ rndlen ( 2 - 4 ) } { extra _ ti...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5165</th>\n","      <td>5167.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: check me out !\\n61 bb\\nhey derm\\nbbbb...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5166</th>\n","      <td>5170.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: hot jobs\\nglobal marketing specialtie...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5167</th>\n","      <td>5171.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: save up to 89 % on ink + no shipping ...</td>\n","      <td>spam</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5168 rows × 3 columns</p>\n","</div>"],"text/plain":["                                name  \\\n","0     0001.1999-12-10.farmer.ham.txt   \n","1     0002.1999-12-13.farmer.ham.txt   \n","2     0003.1999-12-14.farmer.ham.txt   \n","3     0004.1999-12-14.farmer.ham.txt   \n","4     0005.1999-12-14.farmer.ham.txt   \n","...                              ...   \n","5163     5163.2005-09-06.GP.spam.txt   \n","5164     5164.2005-09-06.GP.spam.txt   \n","5165     5167.2005-09-06.GP.spam.txt   \n","5166     5170.2005-09-06.GP.spam.txt   \n","5167     5171.2005-09-06.GP.spam.txt   \n","\n","                                                content category  \n","0               Subject: christmas tree farm pictures\\n      ham  \n","1     Subject: vastar resources , inc .\\ngary , prod...      ham  \n","2     Subject: calpine daily gas nomination\\n- calpi...      ham  \n","3     Subject: re : issue\\nfyi - see note below - al...      ham  \n","4     Subject: meter 7268 nov allocation\\nfyi .\\n- -...      ham  \n","...                                                 ...      ...  \n","5163  Subject: our pro - forma invoice attached\\ndiv...     spam  \n","5164  Subject: str _ rndlen ( 2 - 4 ) } { extra _ ti...     spam  \n","5165  Subject: check me out !\\n61 bb\\nhey derm\\nbbbb...     spam  \n","5166  Subject: hot jobs\\nglobal marketing specialtie...     spam  \n","5167  Subject: save up to 89 % on ink + no shipping ...     spam  \n","\n","[5168 rows x 3 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","id":"1a1c23fd","metadata":{"id":"1a1c23fd"},"source":["Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."]},{"cell_type":"code","execution_count":17,"id":"c447c901","metadata":{"id":"c447c901"},"outputs":[],"source":["import re\n","\n","def preprocessor(text):\n","    # Replace non-alphabetic characters with a space\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    # Convert the text to lowercase\n","    text = text.lower()\n","    return text\n"]},{"cell_type":"markdown","id":"ba32521d","metadata":{"id":"ba32521d"},"source":["Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."]},{"cell_type":"code","execution_count":25,"id":"1442d377","metadata":{"id":"1442d377"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.preprocessing import StandardScaler\n","# Concatenate spam and ham emails\n","# Define the feature matrix X and target variable y\n","X = df['content']  # Assuming 'content' is the column containing email content\n","y = df['category']  # Assuming 'category' is the column containing email categories\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Instantiate a CountVectorizer with the preprocessor\n","\n","# The CountVectorizer converts a text sample into a vector (think of it as an array of floats).\n","# Each entry in the vector corresponds to a single word and the value is the number of times the word appeared.\n","# Instantiate a CountVectorizer. Make sure to include the preprocessor you previously wrote in the constructor.\n","# TODO\n","\n","# Instantiate a CountVectorizer with the preprocessor\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","\n","# Transform the text data into a sparse matrix of token counts\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Scale the data\n","scaler = StandardScaler(with_mean=False)  # Pass with_mean=False for sparse matrices\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Instantiate a LogisticRegression model\n","model = LogisticRegression(max_iter=1000, solver='liblinear')\n","\n","# Train the model\n","model.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = model.predict(X_test_scaled)"]},{"cell_type":"code","execution_count":26,"id":"e26fb2b7","metadata":{},"outputs":[],"source":["\n","\n","# Use train_test_split to split the dataset into a train dataset and a test dataset.\n","# The machine learning model learns from the train dataset.\n","# Then the trained model is tested on the test dataset to see if it actually learned anything.\n","# If it just memorized for example, then it would have a low accuracy on the test dataset and a high accuracy on the train dataset.\n","# TODO\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":27,"id":"0c310774","metadata":{},"outputs":[],"source":["\n","\n","# Use the vectorizer to transform the existing dataset into a form in which the model can learn from.\n","# Remember that simple machine learning models operate on numbers, which the CountVectorizer conveniently helped us do.\n","# TODO\n","\n","# Instantiate a CountVectorizer\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","\n","# Fit the vectorizer on the training data and transform it\n","X_train_transformed = vectorizer.fit_transform(X_train)\n","\n","# Transform the test data using the fitted vectorizer\n","X_test_transformed = vectorizer.transform(X_test)\n"]},{"cell_type":"code","execution_count":28,"id":"e4b453d6","metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"],"text/plain":["LogisticRegression(max_iter=1000, solver='liblinear')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Use the LogisticRegression model to fit to the train dataset.\n","# You may remember y = mx + b and Linear Regression from high school. Here, we fitted a scatter plot to a line.\n","# Logistic Regression is another form of regression. \n","# However, Logistic Regression helps us determine if a point should be in category A or B, which is a perfect fit.\n","# TODO\n","\n","# Instantiate a LogisticRegression model\n","model = LogisticRegression(max_iter=1000, solver='liblinear')\n","\n","# Fit the model on the transformed training data and corresponding labels\n","model.fit(X_train_transformed, y_train)\n"]},{"cell_type":"code","execution_count":29,"id":"8bb5d31d","metadata":{},"outputs":[],"source":["# Validate that the model has learned something.\n","# Recall the model operates on vectors. First transform the test set using the vectorizer. \n","# Then generate the predictions.\n","# TODO\n","# Transform the test data using the same vectorizer\n","X_test_transformed = vectorizer.transform(X_test)\n","\n","# Generate predictions on the transformed test data\n","y_pred = model.predict(X_test_transformed)\n"]},{"cell_type":"code","execution_count":30,"id":"d2adcf4d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9738878143133463\n","Confusion Matrix:\n","[[716  13]\n"," [ 14 291]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         ham       0.98      0.98      0.98       729\n","        spam       0.96      0.95      0.96       305\n","\n","    accuracy                           0.97      1034\n","   macro avg       0.97      0.97      0.97      1034\n","weighted avg       0.97      0.97      0.97      1034\n","\n"]}],"source":["\n","# We now want to see how we have done. We will be using three functions.\n","# `accuracy_score` tells us how well we have done. \n","# 90% means that every 9 of 10 entries from the test dataset were predicted accurately.\n","# The `confusion_matrix` is a 2x2 matrix that gives us more insight.\n","# The top left shows us how many ham emails were predicted to be ham (that's good!).\n","# The bottom right shows us how many spam emails were predicted to be spam (that's good!).\n","# The other two quadrants tell us the misclassifications.\n","# Finally, the `classification_report` gives us detailed statistics which you may have seen in a statistics class.\n","# TODO\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Generate confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","\n","# Generate classification report\n","class_report = classification_report(y_test, y_pred)\n","print(\"Classification Report:\")\n","print(class_report)\n","\n"]},{"cell_type":"markdown","id":"9674d032","metadata":{"id":"9674d032"},"source":["Step 4."]},{"cell_type":"code","execution_count":32,"id":"6b7d78c9","metadata":{"id":"6b7d78c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature names: ['aa' 'aaa' 'aabda' ... 'zzezrjok' 'zzo' 'zzocb']\n"]}],"source":["# Let's see which features (aka columns) the vectorizer created. \n","# They should be all the words that were contained in the training dataset.\n","# TODO\n","\n","# Get the feature names (words) created by the CountVectorizer\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Print the feature names\n","print(\"Feature names:\", feature_names)"]},{"cell_type":"code","execution_count":33,"id":"a0b5363c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 most important features:\n","enron: -1.4896595016061547\n","thanks: -1.4550261634293806\n","attached: -1.3663299947534193\n","doc: -1.3137507690588721\n","daren: -1.2998806892142478\n","pictures: -1.2627505792792995\n","xls: -1.21762068276853\n","neon: -1.1454709373514722\n","deal: -1.1428673773300386\n","hpl: -1.0432552510615658\n"]}],"source":["# You may be wondering what a machine learning model is tangibly. It is just a collection of numbers. \n","# You can access these numbers known as \"coefficients\" from the coef_ property of the model\n","# We will be looking at coef_[0] which represents the importance of each feature.\n","# What does importance mean in this context?\n","# Some words are more important than others for the model.\n","# It's nothing personal, just that spam emails tend to contain some words more frequently.\n","# This indicates to the model that having that word would make a new email more likely to be spam.\n","# TODO\n","# Get the coefficients (importance) of each feature (word)\n","coefficients = model.coef_[0]\n","\n","# Create a dictionary mapping feature names to their coefficients\n","feature_coefficients = dict(zip(feature_names, coefficients))\n","\n","# Sort the feature coefficients by their absolute values (importance)\n","sorted_feature_coefficients = sorted(feature_coefficients.items(), key=lambda x: abs(x[1]), reverse=True)\n","\n","# Print the top N most important features\n","top_n = 10  # You can adjust this number as needed\n","print(f\"Top {top_n} most important features:\")\n","for feature, coefficient in sorted_feature_coefficients[:top_n]:\n","    print(f\"{feature}: {coefficient}\")\n"]},{"cell_type":"code","execution_count":34,"id":"1f65514a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 positive features (spam):\n","no: 0.9292270879886567\n","http: 0.8625289224640907\n","prices: 0.8528204895022904\n","remove: 0.7700346058292749\n","hello: 0.7335981905921252\n","only: 0.6956637893219606\n","removed: 0.6745514409715888\n","here: 0.6728180778770624\n","more: 0.6316008291718711\n","paliourg: 0.6309409022634038\n","\n","Top 10 negative features (ham):\n","enron: -1.4896595016061547\n","thanks: -1.4550261634293806\n","attached: -1.3663299947534193\n","doc: -1.3137507690588721\n","daren: -1.2998806892142478\n","pictures: -1.2627505792792995\n","xls: -1.21762068276853\n","neon: -1.1454709373514722\n","deal: -1.1428673773300386\n","hpl: -1.0432552510615658\n"]}],"source":["# Iterate over importance and find the top 10 positive features with the largest magnitude.\n","# Similarly, find the top 10 negative features with the largest magnitude.\n","# Positive features correspond to spam. Negative features correspond to ham.\n","# You will see that `http` is the strongest feature that corresponds to spam emails. \n","# It makes sense. Spam emails often want you to click on a link.\n","# TODO\n","# Initialize lists to store positive and negative features\n","positive_features = []\n","negative_features = []\n","\n","# Iterate over the sorted feature coefficients\n","for feature, coefficient in sorted_feature_coefficients:\n","    # Check if coefficient is positive (indicating spam)\n","    if coefficient > 0 and len(positive_features) < 10:\n","        positive_features.append((feature, coefficient))\n","    # Check if coefficient is negative (indicating ham)\n","    elif coefficient < 0 and len(negative_features) < 10:\n","        negative_features.append((feature, coefficient))\n","    # Break the loop if both lists are filled\n","    if len(positive_features) == 10 and len(negative_features) == 10:\n","        break\n","\n","# Print the top 10 positive features\n","print(\"Top 10 positive features (spam):\")\n","for feature, coefficient in positive_features:\n","    print(f\"{feature}: {coefficient}\")\n","\n","# Print the top 10 negative features\n","print(\"\\nTop 10 negative features (ham):\")\n","for feature, coefficient in negative_features:\n","    print(f\"{feature}: {coefficient}\")\n","\n"]},{"cell_type":"markdown","id":"d267e7ad","metadata":{"id":"d267e7ad"},"source":["Submission\n","1. Upload the jupyter notebook to Forage."]},{"cell_type":"markdown","id":"LI4u_ZUGToDQ","metadata":{"id":"LI4u_ZUGToDQ"},"source":["All Done!"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"task3.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
